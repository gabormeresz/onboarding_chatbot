services:
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped

  ollama-pull:
    image: curlimages/curl:8.10.1
    depends_on:
      - ollama
    command: [
        "/bin/sh",
        "-c",
        "set -e; \
        echo 'Waiting for Ollama API...'; \
        until curl -s http://ollama:11434/api/version >/dev/null; do sleep 1; done; \
        echo 'Pulling models...'; \
        curl -s http://ollama:11434/api/pull -H 'Content-Type: application/json' -d '{\"name\":\"qwen2.5:7b-instruct\",\"stream\":false}'; \
        curl -s http://ollama:11434/api/pull -H 'Content-Type: application/json' -d '{\"name\":\"bge-m3\",\"stream\":false}'; \
        echo 'Model pulls finished.'"
      ]
    restart: "no"

  ingest:
    build: .
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - PYTHONPATH=/app/src
    depends_on:
      - ollama
      - ollama-pull
    command: [
        "/bin/sh",
        "-c",
        "set -e; \
        echo 'Waiting for embedding model...'; \
        until curl -s http://ollama:11434/api/tags | grep -q 'bge-m3'; do sleep 2; done; \
        echo 'Running ingest...'; \
        uv run python -m onboarding_agent.rag.ingest"
      ]
    restart: "no"

  ui:
    build: .
    ports:
      - "8501:8501"
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - PYTHONPATH=/app/src
      # - LANGSMITH_TRACING=true
      # - LANGSMITH_ENDPOINT=https://api.smith.langchain.com
      # - LANGSMITH_API_KEY=   # put your LangSmith API key here
      # - LANGSMITH_PROJECT=onboarding_agent-docker
    depends_on:
      - ollama
      - ollama-pull
      - ingest
    command: [
        "/bin/sh",
        "-c",
        "set -e; \
        echo 'Waiting for models to be available...'; \
        until curl -s http://ollama:11434/api/tags | grep -q 'qwen2.5:7b-instruct'; do sleep 2; done; \
        until curl -s http://ollama:11434/api/tags | grep -q 'bge-m3'; do sleep 2; done; \
        echo 'Starting Streamlit...'; \
        uv run streamlit run app/streamlit_app.py --server.address=0.0.0.0 --server.port=8501"
      ]

volumes:
  ollama:
